[
  {
    "objectID": "ginzburg.html",
    "href": "ginzburg.html",
    "title": "Quarto demo",
    "section": "",
    "text": "Test of Potts model\n\nusing WGLMakie\nusing LinearAlgebra\nusing CircularArrays\nusing StatsBase: Weights, sample\nusing Random: MersenneTwister\nusing Distributions\nusing MakiePublication\nusing QuartoTools\n\n\nham(J, q::Int, n1::Int, n2::Int) = J*cos(2pi / q * (n1 - n2))\n\nham (generic function with 1 method)\n\n\n\nfunction choose_weights(_s, _q)\n  # choses weights for spin flip\n  weights = Weights(ones(_q)/(_q-1))\n  weights[_s+1] = 0.0 # set probability to choose spin-i again to be zero\n  return weights\nend\n\nfunction spin_flip(_si, _q)\n  # randomly chooses ANOTHER spin\n  # spin = round(Int,_si / 2pi * _q) # convert to integer selection\n\n  w = choose_weights(_si, _q)\n  return sample(0:q-1, w)\nend\n\nspin_flip (generic function with 1 method)\n\n\n\nq = 4  # theta discreteness\nN = 64\n\nn_dims = (N, N)\nkB = 5.0\nJ = -1.0\nspins = CircularArray(sample(0:q-1, Weights(ones(q)/q), n_dims)) \n\n\n# neighbors\nlocs      = eachindex(spins)\nbasis_vec = eachcol(diagm(ones(Int8,length(n_dims)))) # general dimensions\nneighbors = map(_n-&gt;CartesianIndex(_n...), [basis_vec; -basis_vec])\n\nMAXITER  = 100_000\nSTEP     = MAXITER ÷ 1000\ntime     = LinRange(0, 1, MAXITER)\nβmax     = 50.0\nβ        = @. βmax * ((1 + exp(-2*(time-1/2)*10))^-1 - 1/2)  #@. βmin  - 2βmin*time\ntemp     = @. 1.0 / β\n\nspin_vec = zeros(size(spins)..., MAXITER)\n\nfor k in 1:MAXITER\n\n  spin_vec[:,:,k] = spins.data # save state\n\n\n  i = rand(locs) # choose random spin location\n  si = spins[i]\n\n  nearest = i .+ neighbors\n  \n  energy = mapreduce(sj -&gt; ham(J, q, si, sj), +, spins[nearest]) # sum \n\n  si_new = spin_flip(si, q)\n\n  energy_new = mapreduce(sj -&gt; ham(J, q, si_new, sj), +, spins[nearest]) # sum \n  dE = energy_new - energy\n\n  if dE &lt; 0.0\n    spins[i] = si_new\n  elseif rand() &lt; exp(-dE * β[k])\n    spins[i] = si_new\n  end\nend"
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Gallery",
    "section": "",
    "text": "Pre-graduation in Monterrey\n\n\n\n\n\nReflections in Memorial park\n\n\n\n\n\nJames Turrel’s “Twilight Epiphany”\n\n\n\n\n\nMonterrey as seen from El Cerro de la Silla",
    "crumbs": [
      "Gallery"
    ]
  },
  {
    "objectID": "posts/ginzburg.html",
    "href": "posts/ginzburg.html",
    "title": "Landau-Ginzburg",
    "section": "",
    "text": "Essentiaylly, one expands the free energy functional \\(F[m]\\) in \\(m\\) and its gradients while satisfying the proposed symmetries. The coarse-grained extension to the Ising Hamiltonian is given by a Landau-Ginzburg Hamiltonian\n\\[\\begin{equation}\n  H[m] = \\int d\\vec{r}\\left[ \\frac{a}{2}m^2 + bm^4 + \\frac{K}{2} (\\nabla m)^2 +\\dots\\right]\n\\end{equation}\\]\nThis has profound consequences in the statistical theory of fields (using saddle-point approximations to recover Landau’s theory), but I won’t discuss this here (see Kardar’s book). Here, \\(m(\\vec{x})\\) can be interpreted to be order-parameters, but in essence are random fields, i.e. a field with a Boltzmann weight.\nThe above expression is not a kinetic expression. If we want to analyze the relaxation of a system to the equilibrium configuration, we should introduce dynamics of the field \\(m(\\vec{x})\\), which are given by a Langevin equation\n\\[\\begin{equation}\n  \\dot{\\vec{x}}=\\vec{v}(\\vec{x})+\\vec{\\eta}(t),\n\\end{equation}\\]\nwhere the second term, the stochastic velocity, has zero mean. It is direct to see that for our Landau-Ginzburg Hamiltonian we have\n\\[\\begin{align}\n\\partial_t m_i(\\vec{x},t) &=\\mu F_i(\\vec{x})+\\eta_i(\\vec{x},t)\\\\\\\\\n                              &=-\\mu\\frac{\\delta F_i}{\\delta m_i(\\vec{x})}+\\eta_i(\\vec{x},t)\\\\\\\\\n                              &=-\\mu(am_i+4b m_i|\\vec{m}|^2-K\\nabla^2m_i)+\\eta_i(\\vec{x},t)\\\\\\\\\n  \\partial_t \\vec{m}(\\vec{x},t)&=-\\mu(a\\vec{m}+4b \\vec{m}|\\vec{m}|^2-K\\nabla^2\\vec{m})+\\eta_i(\\vec{x},t)\n\\end{align}\\]\nThis last equation is the time-dependent Landau-Ginzburg Hamiltonian. Since this is a nonlinear equation, we resort to numerical methods to solve for the dynamics.\n\n\nThe order parameter or field has no restrictions on its dimensions. Simulating a bidimensional magnetization can be achieved by using complex numbers (say real for (m_x) and imaginary for (m_y)). Lets focus on a scalar field first and see how it evolves in time, first in 2D and then in 3D.\n\n\n\nConsider a complex field (). We set the parameter (a) to be negative for us to have Goldstone modes. We further set (b=1.0 + 1.5i) after some examples I saw after lurking online for the time-dependent LG equation.\nTo simulate the 2D case, we can use Julia’s DifferentialEquations.jl package to solve for a complex field (). The spatial gradients were implemented as convolution operators using custom finite difference kernels, which were generated by the finite_diff_coefficient and get_laplace_kernel functions.\nfunction finite_diff_coefficient(_ord::Int64)\n  if _ord == 2\n    return [1,-2,1]\n  elseif _ord == 4\n    return [-1/12, 4/3, -5/2, 4/3, -1/12]\n  elseif _ord == 6\n    return [1/90, -3/20, 3/2, -49/18,3/2,-3/20,1/90]\n  elseif _ord == 8\n    return [-1/560,8/315,-1/5,8/5,-205/72,8/5,-1/5,8/315,-1/560]\n  end\nend\nThe idea for this is to be able to implement the kernel in any number of dimensions, therefore these functions work for both 2D and 3D (and even 1D if you want to try it out).\nfunction get_laplace_kernel(_dim::Int64,_ord::Int64)\n  base_ker = finite_diff_coefficient(_ord)\n  kernel = zeros(fill(length(base_ker),_dim)...)\n  mid_index = _ord÷2+1\n  kernel[fill(mid_index,ndims(kernel)-1)...,:] .= base_ker # middle of middles\n  \n  perm_ind = collect(permutations(1:_dim))#[1:_dim-1:end]\n  if _dim == 3\n  filter!(_i -&gt; levicivita(_i) &lt; 0, perm_ind)\n  end\n  kernel = mapreduce(_i -&gt; permutedims(kernel,_i),+,perm_ind)\n  \n\n  #two_dim_ker += permutedims(two_dim_ker)\n  return centered(kernel) # return centered version of kernel (0 = middle)\nend\nFinally, the dynamics were defined in the diff_eq function, which computes the time derivative of the field (u) based on the parameters (), (a), (b), (K), and the Laplacian kernel. The periodic boundary conditions were handled using circular convolution from the ImageFiltering package in Julia (thankful I did not have to program the convolutions myself!). This approach may be most efficient for 2D arrays, and makes it simpler to implement and worry about the physics rather than the programming details (like the periodic boundary conditions).\nfunction diff_eq(u , p, t)\n  # dp/dt = mu*F+eta\n  μ,a,b,K,del_sq,dx = p\n  return -μ*(a * u + 4*b*abs2.(u) .* u - K * imfilter(u, del_sq, \"circular\") / dx^2) # periodic boudnary conditions\nend\nThe following heatmap shows the results of an initial random spin configuration (like with the ising model). I believe the sustained oscillations are due to the system exploring the possible configurations. For example, in a 1D Heisenberg model (XY model), one can think of the order parameter () to be degenerate since (for the certain parameters) rotations around the () axis will not affect the free energy. Such states are called low-energy excitations of the system.\n\nOne can check that this absence of low-energy excitations can also happen for a complex-valued field ((b)), as long as (a&gt;0). As seen above, the order parameter is “free” to explore all the configurations around the black line. However, when we initialize the system in a purely real field (or with $a&gt;0$), there are no low-energy excitations present, and instead the system relaxes towards a single minima.\n\n\n\n    \n    \n    \n    \n    \n\n\n\n\n\n\nI have already explained that the above functions work for ay dimension. However, the only change is that in 3D, multiplication becomes a bit more expensive, and we’re dealing with even more data-points. It is convenient to use a simple workhorse for solving the dynamics, the Euler method with small time step. We also include a random (Gaussian) velocity in each time-step as to account for random fluctuations.\nHere, rather than starting with a random distribution, we start with a Gaussian field at the origin. As expected, low-energy excitations emerge.\n\n\n\nA friend interested in phase separation (and more experience in physics than I) suggested I read about the Cahn-Hilliard model after showing him the simluations above. Modifying the above code is trivial (just includes another convolution); below is a simulation of such separation using a balanced mixture (following the Cahn-Hilliard model), when constrained to a box (i.e. no periodic boundary conditions).\n\nThere is a great discussion on so called dissipative stochastic models (i.e. kinetic Ising models) in the wonderful book by Chaikin and Lubensky. It should be fun to implement the remaining models in the A-J list shown in page 467.",
    "crumbs": [
      "Posts",
      "Landau-Ginzburg"
    ]
  },
  {
    "objectID": "posts/ginzburg.html#solving-the-time-dependent-landau-ginzburg-equation",
    "href": "posts/ginzburg.html#solving-the-time-dependent-landau-ginzburg-equation",
    "title": "Landau-Ginzburg",
    "section": "",
    "text": "The order parameter or field has no restrictions on its dimensions. Simulating a bidimensional magnetization can be achieved by using complex numbers (say real for (m_x) and imaginary for (m_y)). Lets focus on a scalar field first and see how it evolves in time, first in 2D and then in 3D.",
    "crumbs": [
      "Posts",
      "Landau-Ginzburg"
    ]
  },
  {
    "objectID": "posts/ginzburg.html#d-treatment",
    "href": "posts/ginzburg.html#d-treatment",
    "title": "Landau-Ginzburg",
    "section": "",
    "text": "Consider a complex field (). We set the parameter (a) to be negative for us to have Goldstone modes. We further set (b=1.0 + 1.5i) after some examples I saw after lurking online for the time-dependent LG equation.\nTo simulate the 2D case, we can use Julia’s DifferentialEquations.jl package to solve for a complex field (). The spatial gradients were implemented as convolution operators using custom finite difference kernels, which were generated by the finite_diff_coefficient and get_laplace_kernel functions.\nfunction finite_diff_coefficient(_ord::Int64)\n  if _ord == 2\n    return [1,-2,1]\n  elseif _ord == 4\n    return [-1/12, 4/3, -5/2, 4/3, -1/12]\n  elseif _ord == 6\n    return [1/90, -3/20, 3/2, -49/18,3/2,-3/20,1/90]\n  elseif _ord == 8\n    return [-1/560,8/315,-1/5,8/5,-205/72,8/5,-1/5,8/315,-1/560]\n  end\nend\nThe idea for this is to be able to implement the kernel in any number of dimensions, therefore these functions work for both 2D and 3D (and even 1D if you want to try it out).\nfunction get_laplace_kernel(_dim::Int64,_ord::Int64)\n  base_ker = finite_diff_coefficient(_ord)\n  kernel = zeros(fill(length(base_ker),_dim)...)\n  mid_index = _ord÷2+1\n  kernel[fill(mid_index,ndims(kernel)-1)...,:] .= base_ker # middle of middles\n  \n  perm_ind = collect(permutations(1:_dim))#[1:_dim-1:end]\n  if _dim == 3\n  filter!(_i -&gt; levicivita(_i) &lt; 0, perm_ind)\n  end\n  kernel = mapreduce(_i -&gt; permutedims(kernel,_i),+,perm_ind)\n  \n\n  #two_dim_ker += permutedims(two_dim_ker)\n  return centered(kernel) # return centered version of kernel (0 = middle)\nend\nFinally, the dynamics were defined in the diff_eq function, which computes the time derivative of the field (u) based on the parameters (), (a), (b), (K), and the Laplacian kernel. The periodic boundary conditions were handled using circular convolution from the ImageFiltering package in Julia (thankful I did not have to program the convolutions myself!). This approach may be most efficient for 2D arrays, and makes it simpler to implement and worry about the physics rather than the programming details (like the periodic boundary conditions).\nfunction diff_eq(u , p, t)\n  # dp/dt = mu*F+eta\n  μ,a,b,K,del_sq,dx = p\n  return -μ*(a * u + 4*b*abs2.(u) .* u - K * imfilter(u, del_sq, \"circular\") / dx^2) # periodic boudnary conditions\nend\nThe following heatmap shows the results of an initial random spin configuration (like with the ising model). I believe the sustained oscillations are due to the system exploring the possible configurations. For example, in a 1D Heisenberg model (XY model), one can think of the order parameter () to be degenerate since (for the certain parameters) rotations around the () axis will not affect the free energy. Such states are called low-energy excitations of the system.\n\nOne can check that this absence of low-energy excitations can also happen for a complex-valued field ((b)), as long as (a&gt;0). As seen above, the order parameter is “free” to explore all the configurations around the black line. However, when we initialize the system in a purely real field (or with $a&gt;0$), there are no low-energy excitations present, and instead the system relaxes towards a single minima.",
    "crumbs": [
      "Posts",
      "Landau-Ginzburg"
    ]
  },
  {
    "objectID": "posts/ginzburg.html#d-treatment-1",
    "href": "posts/ginzburg.html#d-treatment-1",
    "title": "Landau-Ginzburg",
    "section": "",
    "text": "I have already explained that the above functions work for ay dimension. However, the only change is that in 3D, multiplication becomes a bit more expensive, and we’re dealing with even more data-points. It is convenient to use a simple workhorse for solving the dynamics, the Euler method with small time step. We also include a random (Gaussian) velocity in each time-step as to account for random fluctuations.\nHere, rather than starting with a random distribution, we start with a Gaussian field at the origin. As expected, low-energy excitations emerge.\n\n\n\nA friend interested in phase separation (and more experience in physics than I) suggested I read about the Cahn-Hilliard model after showing him the simluations above. Modifying the above code is trivial (just includes another convolution); below is a simulation of such separation using a balanced mixture (following the Cahn-Hilliard model), when constrained to a box (i.e. no periodic boundary conditions).\n\nThere is a great discussion on so called dissipative stochastic models (i.e. kinetic Ising models) in the wonderful book by Chaikin and Lubensky. It should be fun to implement the remaining models in the A-J list shown in page 467.",
    "crumbs": [
      "Posts",
      "Landau-Ginzburg"
    ]
  },
  {
    "objectID": "posts/ginzburg.html#simulating-the-classical-potts-model",
    "href": "posts/ginzburg.html#simulating-the-classical-potts-model",
    "title": "Landau-Ginzburg",
    "section": "Simulating the classical Potts model",
    "text": "Simulating the classical Potts model\n\nFor larger and larger \\(q\\), we see that it starts to resemble what we had before with the LG model. Because this is a classical model, fluctuations are due to temperature only.",
    "crumbs": [
      "Posts",
      "Landau-Ginzburg"
    ]
  },
  {
    "objectID": "posts/waves.html",
    "href": "posts/waves.html",
    "title": "Waves",
    "section": "",
    "text": "Wave simulations are essential in optics and electrodynamics. Here are some of the simulations I’ve done for wave propagation using finite-difference schemes.\n\n\nDiscrete wave equation\nIn general, using a FDTD scheme is just a fancy way of discretizing a differential operator. In this case, we want to discretize the following equation,\n\\[\\begin{equation}\\partial^2_{t}\\psi(\\mathbf{r},t)-\\alpha^2{\\Delta}\\psi(\\mathbf{r},t)=0\\quad (+\\phi),\\end{equation}\\]\nwith \\(\\Delta\\) being Laplace’s operator. One can also add a source term, \\(\\phi\\), but this does not cause any major differences when discretizing the homogeneous equation. Wikipedia’s page on the Discrete Laplace Operator gives a great insight into how the method works for \\(\\mathcal{O}(h^2)\\) error. Long story short, we deal with a generalization of the central finite difference method for higher dimensions. In 1D, one has the coefficients \\((1,-2,1)\\), which discretize \\(\\partial_x^2\\). In 2D, the same coefficients apply, but now in the perpendicular direction. In a cartesian grid, this translates to having a column vector of coefficients \\((1,2,1)^T\\). For 3D, the same methodology applies. Having the connection between the differential operator and its discrete version \\(\\Delta_{x,y,z}\\rightarrow \\mathbf{D}_{i,j,k}^2\\), one can infer the structure of the kernel to be used,\n\\[\\begin{equation}\\Delta\\equiv\\partial^2_x+\\partial^2_y+\\partial_z^2\\end{equation}\\]\n\\[\\begin{equation}\\mathbf{D}_{i,j,k}^2\\equiv\\mathbf{D}^2_i+\\mathbf{D}^2_j+\\mathbf{D}^2_k\\end{equation}\\]\n\\[\\begin{equation}\n\\mathbf{D}^2_{(i,j,k=-1)}=\n\\begin{pmatrix}\n0&0&0\\\\\n0&1&0\\\\\n0&0&0\\\\\n\\end{pmatrix},\n\\end{equation}\\] \\[\\begin{equation}\n\\mathbf{D}^2_{(i,j,k=0)}=\\begin{pmatrix}0&1&0\\\\1&-6&1\\\\0&1&0\\end{pmatrix},\n\\end{equation}\\] \\[\\begin{equation}\n\\quad\\mathbf{D}^2_{(i,j,k=1)}=\\begin{pmatrix}0&0&0\\\\0&1&0\\\\0&0&0\\end{pmatrix}\\end{equation}\\]\nIn fact, image processing software and tools use this types of convolution kernels for filters and other algorithms. For hicher precision, one replaces the centered difference with its respective weights (these are listed in this page). Although more precise, it has the disadvantage of requiring a bigger stencil and having wider boundaries.\nThe above formulation is an alternative to what is done in realistic codes, where one instead creates a super-matrix that corresponds to the kronecker product of the derivative operators, i.e.\n\\[\\begin{equation}\nD_{x,y}=D_x\\otimes I_{N_y} + I_{N_x} \\otimes D_y\n\\end{equation}\\]\n\n\n    \n    \n\n\nFor example, for the eigenmodes of an L-shaped cavity,\n\nnx = 120; ny = 111\n\nx1 = LinRange(-1, 1, nx)\nx2 = LinRange(-1, 1, ny)\n\ncond = [!((x &lt; 0) && (y &lt; 0)) for x in x1, y in x2]\n\ndx1 = x1[2]-x1[1]\ndx2 = x2[2]-x2[1]\n\nDx = diff2mat(nx, dx1, false)\nDy = diff2mat(ny, dx2, false)\n\nD = kron(speye(ny), Dx) + kron(Dy, speye(nx))\n\nDi = D .* vec(cond)\n\nvals, vecs = eigs(Di, which = :SR, 1e-10,maxiter=1500, nev = 10)\n\n\nplots = []\nfor j in 1:6\n  psi = reshape(vecs[:,j], nx, ny) \n  psi[.!(cond)] .= NaN \n  p = Plots.heatmap(abs.(psi), colormap = :plasma, colorbar=false, framestyle=:none,aspect_ratio=1) \n  push!(plots, p)\n  \nend \nplot(plots..., layout = (2, 3))\n\n    \n    \n\n\n\n\nAbsorbing Boundary Conditions\nWhen using Finite Difference Schemes alongside ABCs, one must remember to apply it to the entire boundary. For example, when using a FDM of order \\(O(h^6)\\), the region on the boundary has element-size 3. The absorbing boundary conditions (ABCs) implemented what Gerrit Mur derived in a 1981 IEEE Transactions article. Although Perfectly-Matched-Layer (PML) method is far more robust than traditional ABCs, it has been coded by various software developers (such as in COMSOL) far better than I could. As a rule of thumb, use a PML thickness similar to half the largest wavelength.\n\n\n3D example\nAt the top of the page I present a 3D solution of the discrete wave equation, using an approximation with error \\(\\mathcal{O}(h^6)\\), with absorbing boundary conditions.",
    "crumbs": [
      "Posts",
      "Waves"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Below are some of my recent projects and animations. Click on any of them to learn more and see the code.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLandau-Ginzburg\n\n\nSymmetries and mean-field theories\n\n\n\n\n\n\n\n\n4/9/25\n\n\n\n\n\n\n\n\n\n\n\n\nWaves\n\n\nFinite difference methods\n\n\n\n\n\n\n\n\n6/11/23\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Alberto Ruiz Biestro",
    "section": "",
    "text": "Email\n  \n  \n    \n     Github\n  \n\n  \n  \nCurrently a graduate Physics & Astronomy student @ Rice University. I completed my BSc. in Engineering Physics at Monterrey Institute of Technology, where I did my research in computational optics and quantum billiards. My fields of interest vary from quantum computing to photonics, computational physics, and more. Proficient in Julia, MATLAB, and Python. Know my way around Mathematica.\n\n\n\nRice University, TX\nPhD in Physics & Astronomy\n2024 – Present\nMonterrey Institute of Technology NL, Mexico\nBSC in Engineering Physics\n2020 – 2024",
    "crumbs": [
      "Alberto Ruiz (Bio)"
    ]
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Alberto Ruiz Biestro",
    "section": "",
    "text": "Rice University, TX\nPhD in Physics & Astronomy\n2024 – Present\nMonterrey Institute of Technology NL, Mexico\nBSC in Engineering Physics\n2020 – 2024",
    "crumbs": [
      "Alberto Ruiz (Bio)"
    ]
  }
]